{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.3%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "93.0%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Date: 04 Jan 2020\n",
    "\n",
    "Python version:      3.7\n",
    "Tensorboard version: 1.14.0\n",
    "PyTorch version:     1.2.0\n",
    "\n",
    "@author: Maksim Lavrov\n",
    "\n",
    "Only one flexible layer with 12 output filters\n",
    "But with the avgpool instead of maxpool in the flexilayer\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# imports\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# transforms\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# datasets\n",
    "trainset = torchvision.datasets.FashionMNIST('./data',\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=transform)\n",
    "testset = torchvision.datasets.FashionMNIST('./data',\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=transform)\n",
    "\n",
    "#create a dataset subset to reduce training time\n",
    "\n",
    "sampler_train = list(range(0, len(trainset), 1000))\n",
    "sampler_test = list(range(0, len(testset), 1000))\n",
    "trainset_samp = torch.utils.data.Subset(trainset, sampler_train)\n",
    "testset_samp = torch.utils.data.Subset(testset, sampler_test)\n",
    "\n",
    "#set size of batch and learning rate\n",
    "batch_size=4\n",
    "lr=0.001\n",
    "\n",
    "# dataloaders\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                        shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                        shuffle=False, num_workers=2)\n",
    "\n",
    "# constant for classes\n",
    "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xavier threshold initialization\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Conv2d:\n",
    "        torch.nn.init.xavier_uniform(m.threshold1)\n",
    "        m.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlexiLayer(nn.Conv2d):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
    "                 padding=0, dilation=1, groups=1,\n",
    "                 bias=True, padding_mode='zeros'):\n",
    "        kernel_size = kernel_size\n",
    "        stride = stride\n",
    "        padding = padding\n",
    "        dilation = dilation\n",
    "        super(FlexiLayer, self).__init__(\n",
    "            in_channels, out_channels, kernel_size, stride, padding, dilation,\n",
    "            groups, bias, padding_mode)\n",
    "        \n",
    "        \n",
    "        self.threshold1 = nn.parameter.Parameter(torch.randn((4, 12, 24, 24)), requires_grad=True)\n",
    "        self.apply(init_weights)\n",
    "        self.memorized = self.threshold1.clone()\n",
    "        self.memorized_1loop = []\n",
    "            \n",
    "    def forward(self, t):\n",
    "        \n",
    "        t_1 = F.relu(F.conv2d(t, self.weight)) # get convolution result\n",
    "        t_2 = F.avg_pool2d(t, kernel_size=5, stride=1) # get avg result with the same kernel size\n",
    "       #t_2 = torch.cat((t_2, t_2), 1)\n",
    "        m = nn.Sigmoid()\n",
    "        condmax = torch.sub(t_2, self.threshold1)\n",
    "        condconv = torch.sub(t_2, self.threshold1)\n",
    "        t_2 = m(condmax*50)*t_2 # \n",
    "        t_1 = m(condconv*(-50))*t_1 # \n",
    "        t = torch.add(t_2, t_1)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "        \n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flex1 = FlexiLayer(in_channels=1, out_channels=12, kernel_size=5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=12*12*12, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "        self.out = nn.Linear(in_features=60, out_features=10) # number of classes\n",
    "\n",
    "    def forward(self, t):\n",
    "        \n",
    "        # (2) Flexible layer\n",
    "        t = self.flex1(t)\n",
    "        \n",
    "        # (4) hidden linear layer\n",
    "        t = F.relu(self.fc1(t.reshape(-1, 12 * 12 * 12)))\n",
    "        \n",
    "        # (5) hidden linear layer\n",
    "        t = F.relu(self.fc2(t))\n",
    "        \n",
    "        # (6) output layer\n",
    "        t = self.out(t)\n",
    "        \n",
    "        return t\n",
    "\n",
    "net = Net()\n",
    "memorized = net.flex1.memorized\n",
    "#before training\n",
    "imgplot = plt.imshow(net.flex1.threshold1.detach().numpy()[1,1,:] - memorized.detach().numpy()[1,1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "# helper function to show an image\n",
    "# (used in the `plot_classes_preds` function below)\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "def images_to_probs(net, images):\n",
    "    '''\n",
    "    Generates predictions and corresponding probabilities from a trained\n",
    "    network and a list of images\n",
    "    '''\n",
    "    output = net(images)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, preds_tensor = torch.max(output, 1)\n",
    "    preds = np.squeeze(preds_tensor.numpy())\n",
    "    return preds, [F.softmax(el, dim=0)[i].item() for i, el in zip(preds, output)]\n",
    "\n",
    "\n",
    "def plot_classes_preds(net, images, labels):\n",
    "    '''\n",
    "    Generates matplotlib Figure using a trained network, along with images\n",
    "    and labels from a batch, that shows the network's top prediction along\n",
    "    with its probability, alongside the actual label, coloring this\n",
    "    information based on whether the prediction was correct or not.\n",
    "    Uses the \"images_to_probs\" function.\n",
    "    '''\n",
    "    preds, probs = images_to_probs(net, images)\n",
    "    # plot the images in the batch, along with predicted and true labels\n",
    "    fig = plt.figure(figsize=(12, 48))\n",
    "    for idx in np.arange(4):\n",
    "        ax = fig.add_subplot(1, 4, idx+1, xticks=[], yticks=[])\n",
    "        matplotlib_imshow(images[idx], one_channel=True)\n",
    "        ax.set_title(\"{0}, {1:.1f}%\\n(label: {2})\".format(\n",
    "            classes[preds[idx]],\n",
    "            probs[idx] * 100.0,\n",
    "            classes[labels[idx]]),\n",
    "                    color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))\n",
    "    return fig\n",
    "\n",
    "def get_num_correct(preds, labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAB5CAYAAAAtfwoEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAagElEQVR4nO2debBdRbWHvyUzRAxhiEwGUAoCyGRklhkHRIMFWEEeREmZiubJ9CwSnqXIEwfkKUMxWJEgqAgvzCkMSgyJCAgYpjCEIUwhEAhImJWx3x/nrL6/E/bOucO5w9l3fVVU1u2zz9699+7T9Pr16tWWUiIIgiCoDh/q7woEQRAErSU69iAIgooRHXsQBEHFiI49CIKgYkTHHgRBUDGiYw+CIKgYPerYzezzZvawmS0ws8mtqlQQBEHQfay7cexmtgLwCHAAsAj4B3B4SunB1lUvCIIg6Cor9uC7OwELUkqPA5jZZcBooLRjHzJkSFp77bV7cMkgCILBx8KFC19MKa3b2eN70rFvCDwtfy8Cdl72IDMbD4wHGDZsGJMmTerBJYMgCAYfEydOfKorx/dEY7eCsg/oOimlKSmlUSmlUUOGDOnB5YIgCILO0JOOfRGwsfy9EfBsz6oTBEEQ9JSedOz/ADY3s03NbGVgDDC9NdUKgiAIuku3NfaU0rtm9p/An4EVgAtTSg909Tzf/va3u1uFQct5551XWD4QnuW7776b7RkzZgAwdOjQXPbOO+9k+4EHOprLyJEjATjggAN6u4oNFD3LgfAc242B3CbbjbJn2RV6MnlKSmkGMKPHtQiCIAhaRqw8DYIgqBg9GrEHg5e//e1v2Z43b16233jjjWy7G14WDbXffvtl+/vf/z7QId8AjB07Ntvbb799D2scBIOHGLEHQRBUjOjYgyAIKkZIMUGXuOWWWwC48MILc9nnPve5bE+cOLFb5/3Rj34EwK233prLTj311Gzvv//+2Z4wYUK3rhEEg4UYsQdBEFSM6NiDIAgqRkgxQUZTOJt1pAJ67LHHsn3KKacAjREtY8aMKTzf+++/D8CHPtT58cNuu+2W7YULF2Z76tSp2d59990B+OQnP9m07kEwGIkRexAEQcWIEXuQ8RE2wAorrJDtpUuXZnurrbYC4Nhjj216vq6M1H3EraPtww47LNv33HNPtq+55hqgccSu34vRezDYiRF7EARBxYiOPQiCoGKEFBNkVH558803s33xxRdn+/jjjwdg1VVXbem1iyQTrc+nP/3pbM+dO7fL5woGF0VynJap7KiSYVHbee+997L973//O9trrLFGayrbC8SIPQiCoGJExx4EQVAxQooJCrn88suzrSkDNttss+V+r7ciUg455JBsf+xjHwNgzpw5uWzvvfdu2bWC9kTbXhHaHlXmK0LXUBxzzDHZ3muvvbLtm8Lob0IlSpV7/HunnXZaLttjjz06XfeuEiP2IAiCihEdexAEQcUIKSbInHvuudl+9tlns60RKUW0wo0sWqBUhtdH99MMKWZwUdTmytpOUdvS6BaVZe666y6gcQHeGWecke31118/2zfddBMAs2fPzmW65+8WW2yRbU/BMW3atFymUkyrI7majtjN7EIzW2Jm90vZMDObaWaP1v9dq6W1CoIgCLpNZ0bsFwHnAL+VssnArJTSz8xscv3vSa2vXv/y1ltvZXuVVVZp2XkH6pL3f/7zn9nWe7/vvvuyffDBB/fKtbvyHC655BKg9bH0QftQ1F7KPEc/Vr1QHZHvsssu2fZjvv71r+cyjWN//vnnsz1q1CgAXnvttVymI/prr7022y+//DIAH/7whwvr2GqajthTSjcBLy1TPBrwVSsXA73zaw+CIAi6THcnT4enlBYD1P9dr+xAMxtvZnPNbO7rr7/ezcsFQRAEnaXXJ09TSlOAKQAjRoxobbBmDymaVHn00Uezfc4552T76KOPBmC77bbr8XWbuZG9Jc80u4be+9Zbb51tXTrtqQZWX3315Z5Lr9eK+3nwwQezfcMNNwCN8tgTTzyR7U033bTH1+sPdOJtxRWLf5qe3uGvf/1rLtNtCosoWz5fZYra3B/+8Ids+1oIaJRH1lqrNl04dOjQXHbHHXdkW9v9lltuCcDw4cNzmb63V155JdvvvPMO0CgHdeZ9d5fuvuXnzWx9gPq/S1pXpSAIgqAndLdjnw6MrdtjgWuXc2wQBEHQhzQd/5vZpcDewDpmtgg4GfgZMM3MxgELgcPKz9D/dCXOWt2jj370o9m+8847AVhttdVymbq4GrPaTHrQWXQ/VucfNNpDXcJWLzt2Hn/8cQDefvvtXKZRMdtuu222//73vwONW+OV0RUJpki20S35Lrjggmzvs88+ANx77725TKWJdpViytzxXXfdNdu33XYb0Cib6XMokgpbIb9o2zjxxBMBOPPMM3t83u5S9FvQ6BV9ls899xwAP/jBD3LZCSeckG19Zh7frpLJU089lW3d3MXbnG8+A/CZz3wm2xMmTMi2vy/93fz0pz/NtvY1raBpx55SOrzko+a/7CAIgqDPGRwzKUEQBIOIyqUUKHLpuyIJjBw5Mtsf//jHs/2nP/0JgLPPPjuXvfjii9nef//9sz169GgA1l133cJrqBTjs+zqWuoiCJViehpdUvb9Sy+9FGh02XWjjWeeeSbbK620EgAvvdSxtGHYsGGF5+1pVIzXCxrlIM+mp7KY1qdd0X1dNaPmxIkTsz1z5kygUR548skns+2yA3QsvPnIRz7SrfqoPDhv3rxsL1iwAOg9abAraNsqk7Jc2tTFdVdffXW2zzrrrGy7XLP77rvnMs30qM/S5anp06d/oAzgK1/5SrZdTlNZTTOofuc73ymse3eJEXsQBEHFGLAj9mYx12WfFx37xhtvZFtjn5vFjq688srZ9kkPHU1r8h8dhftS4kMPPTSXvfrqq9lec801s+2TNFpvnQhUr6HVsa7OI488AnSMxqFxFP7QQw9l20fLngAJGkc36qV0ZaTux+qE6QYbbJBtnVzyCW4dpevEWW9T1vaaJabSdqiT9J4MSpOteaw+NHqRHq//9NNP5zIdJS5evDjbixYtAuBTn/pULtPc4dqevD7qnflEOTTGfe+0005A42+hjK7kSO8KRd+bOnVqtnXdgz9XTXKn39el/+PGjQMa29Opp56a7c033zzb7mHp5Kt6mfouPGGdjswnTerIwhIj9iAIgmC5RMceBEFQMQasFNPMRSv73ONQdYm5upzrrdeR1qYrk0ou4XhGN2h093RyyV0/n2SCxknXhx9+ONu//W0taaZOxLo7uGzdW4kud/Z67rvvvrnMl0BDoyzj7r0uw9aJKI0h963DOoPLPVdeeWUuc5cfGiUCr5s+s0984hOdvlZPKWt7zdrs1772tWyPGDEi2zNmzAAan7PGUXuOcOiQTIYMGZLLdIJ94403zrZnFPQ1GAC33nprtnWy3M+rE9Jq6/Vcoly6dOkH7rGMZpKVluv6ED226LcwefLkbOsk5nXXXZdt/81rSgGVVLTN/uUvfwEaJzZ//OMfZ3vKlCnZ9i3zdHJ17Nix2V5nnXWy7etCVFbrzdxZMWIPgiCoGNGxB0EQVIwBK8UUoS6aLnnX2Xmfid5mm21ymcoGLiVAhwuly4TLcDdwhx12yGU6c66un7uoWodf//rX2fZMkQA33ngjAN/61rea1qGVuPsPHdEnX/rSl3LZ7373uw98Dh0xwXfffXcuUwlHJZNf/OIXQGMctqZLUHfXpTOtg6c6APjXv/6VbY/910gZjQzpLYri8lUq0DbpUVIqF6kcpxFXfg6V6DSCRmUx/55Gd2nGQZVXfMs3/Vyfo0Zy+W9EMxXqlnEq3bks05moGH9W+pzKIpj8enrdMjxGXNuTpp3YZJNNsu1RX5qGQdcB6BoJf1bf/OY3c9kPf/jDbH/3u9/NtkfLaEoHbRsaOeb9hj4HXSuiUVKtIEbsQRAEFSM69iAIgorRVlKMujkacaIRBu4mqps/a9asbGtUwDXXXAN0f3n8kiUdaeiPOOKIbLtL6FkIAQ455JBsH3TQQdn29AOKut7qWrdykwSVWjyiRO9NZ+x14VJRXebMmZPtvfbaK9vuJmsWQI1s0AUcLqXophF77rlntjWayV1rjdRotSvr6HPw5foq56k7re/NN2zQ+1UZROUGj+TS7yvaJl3uUVlSZRJ9Dv6OVPbxeunniu57q+1bz+vPRKWNMooiYJpFevnzgMboIK2by3Aa3XL66adn+8gjj8y2Pyt9/iqT6jt2KUrlFV2gpzKqL8xTqVI5/PCO/Il+7euvvz6X6QY2Kju2ghixB0EQVIwBO2IvWratI8rx48dnWyfsPIZcJ0Q1D7OP9qAj5nfttdfOZRqb2ywBl2+NBY27k/tEqE6G6ShFY651YsspGiG3Gh05eJzz/Pnzc5nGMOtIx0dT+rmONDXm2kdTOqLRSb+NNtoo2z556kmuoHFCWidVP/vZzwKNk3cPPPBAtnUUXfR8m6EjYB1Z+31orLhOmOqxnphLR5/6nPQde9vQCWA9l070OfpOdKJVR/LuMel19bw6YvTJU53Y1PavXoNPTKoX5UnylqXod6OTti+88EK2fT2EtgH1Kr74xS9m+7DDaltAaC50/R3/5Cc/ybZ709qGdtxxx2zrb/6yyy4DGterKJpmwQMI9Hdz3HHHZVuf78033ww0brOnSoP+blpBjNiDIAgqRnTsQRAEFWPASjHNlmfrMl/POw0drqZKH3/84x+zrRN97tKffPLJuez888/P9kknnbTcOqjbq9KESwzqOqprp7HCvuzb/4XGyVxNjaDbbnUHPZfibqnKM3o/ik/ElcWNq/TgE9x6v5qVUF19zzr4jW98I5dpRkx9b/4+d95551ymk2wqj3RHitFJXcVlpLJl9yqv+DJ1lT60vajE0KytF01A6rnKjvVr6LFq63X9WP1+2dJ/l4E6s3bAz6HHqjT6+9//Pts+Mamfa3vSyVM/r76r3XbbLduazsL3SjjqqKNymeZF14yXPuE5bdq0XDZ37txs63tzCUwnp88555xs6+SyT7xrP6BZXlWWacXeAk1H7Ga2sZnNNrP5ZvaAmR1bLx9mZjPN7NH6v2s1O1cQBEHQ+3RGinkX+K+U0khgF2CimW0FTAZmpZQ2B2bV/w6CIAj6mc5sZr0YWFy3XzOz+cCGwGhg7/phFwNzgEkFp1guuqmEbi6gMczu1mpqAF2ercvjfQm3usC+szvAn//852y7a61Lhm+//fZsa/ZGjzBQ11HlII2b9XNotI66lFtvvXW23X3UTHAaxaCSksaIdwfdaEBTIzgaEaFLtdX9dJdaj9VIF5VBPA66LEpFZQGPjlC3Vj9X17kogkbr2N2t4Jwtttgi2xp547a6ynpvWnffcETrpTJUkUSjcoW2l6Il9iqTlMkyRTKUHqvtzO9ZIzk0ekVlMb8PjUIpwyUIlR3OOOOMbP/85z/PtkeRaZmmZFDpzu/DNymBxveuz+c3v/kNAAceeGAu0w1hNN7c26FGbOn71vfi9dH3rr/X7bffPtseQaS/D80I29M2uyxdmjw1s02AHYDbgeH1Tt87//VKvjPezOaa2dzeTFMZBEEQ1Oh0x25mQ4ArgeNSSq82O95JKU1JKY1KKY3SlYJBEARB79CpqBgzW4lap35JSumqevHzZrZ+Smmxma0PLCk/QzkqL+jejLqM15dwqzzge5AC3HLLLdn2yAR1mXRBkMo9nl6gbEGKRjz4wieVUXSxgcpELquo+9kKerqvp7r6umO7o1EH+hw02sNdSV0Uoy6yyjJe37KNE9S9L7quuq16vQ033BBoXCCjEpp+ryy6Z3lotIhmkFTb0XsoklJUDtHl71qvogV4ZfVptq9qUVSMPtOiz7Xu2sZUrtAFeL4oSCUI3/9zWfy3qekJVPrQFAe+zP+iiy7KZbr4RyVVL9fFYvfff3+2v/rVr2bbJVGXZKAx8kZ/x763q75rjVjR37S3dZW0tN9RhcIjiVRy1cVMutBKI8C6S2eiYgyYCsxPKf1SPpoO+HYhY4Frl/1uEARB0Pd0ZsS+O3AkcJ+Z+f+W/xv4GTDNzMYBC4HDeloZlWrKlvQ6GrNatMN32ZJrTWbko1YdcbYi0ZZPqOkoRUdVWoeiuHsd4Wrd9f/23UFHlxoDXoTWt2iiVD/XEaE+d/ew9PtaB71PH/3peXW0q96ax/zr8/CRFjQuU9fy3qDVXll/oZ5WK/H2q+/kiiuuyLYm4/IJSx3Fq4esdhE66a2jZU/JoOtAdL2KBjn4JKb+7soSufkx6s1oHdyzhI62qu24N+lMVMzNQNkKiv1KyoMgCIJ+IlIKBEEQVIwBm1Kgp5Qt++4LXELoznL2Zemp/KLoBG/RdoCzZ8/Otm89Bo2Tgm7rvenkkbqaLquoW6sush7r59C4cJ2IKsp4qdfViVR1rXtbigmWz5gxY4DG9R8az6/v0OWaov0VoDh9g7YLlQQ11YAHT/hWmNAoCWr7dPlUz6uSiv5uvM11V8YqSzHRCmLEHgRBUDGiYw+CIKgYlZVigg+iG14UxZCrq6uSiEYFuIuq0QEa5aPHutSibq3a6n66G61urbqqWh+PstHva3ZMlXCC/sXf57hx43KZvkvdjMblE91YROVDlf+K5A/dLlNTQXjb0naqS/g1Zt0zPWqZykHdycSpFGXU7A1ixB4EQVAxomMPgiCoGCHFDCJ0Kffll1+ebc+Qp9Ev6n5qyoWijTbKpBY/Rt1mdUU1IsLtovQFy+IurH5fsxLq/pv77LNP4TmCvsGlibKFZ5pl1G19r9rOVErx8rL0DyoJuq0pEMqyl3b2ftTWOjSTavqKGLEHQRBUjBixDyJ8d3mAbbfdNtueE1+X/utWczpq8mPKcrfrbvY+otFRjo7IdXLUz6fH6sitKOe1TqzpxNnIkSMJBgbdGcGWeYADgYE4Oi8iRuxBEAQVIzr2IAiCihFSzCBF49B9Z3XdjnDp0qXZVpfTZRmVX3SiSmOQm6VD0CXgntFPJ8jKMky6hKNSjS4X1230gmAwEiP2IAiCihEdexAEQcUIKWaQctddd2Xbs+npBge6LZpKJi5/aIy5xqnrtl6erW/48OG5TKMcNJ7ZI1yK4o/1utCxJF3TIuhGDrrFYhAMRmLEHgRBUDGiYw+CIKgYIcUMUiZMmJBt32H+ueeey2W6IYZuROBZFDWboi5guuqqq7Lt+4FqBE5ZpIuX6/JulWJ04ZJfW+UZ3y8TGjdGCILBSNMRu5mtamZ3mNm9ZvaAmZ1SL9/UzG43s0fN7P/MbOVm5wqCIAh6H+tE/mAD1kgpvW5mKwE3A8cCJwBXpZQuM7NfAfemlM5f3rlGjBiRJk2a1KKqB0EQDA4mTpx4Z0ppVGePbzpiTzXcL1+p/l8C9gWuqJdfDBzcxboGQRAEvUCnJk/NbAUzuwdYAswEHgNeTin5MsFFQKGwaWbjzWyumc1V3TYIgiDoHTrVsaeU3kspbQ9sBOwEFKXPK9R0UkpTUkqjUkqjNMd3EARB0Dt0KdwxpfQyMAfYBRhqZh62sBHwbNn3giAIgr6jM1Ex65rZ0Lq9GrA/MB+YDRxaP2wscG1vVTIIgiDoPJ2JitmW2uToCtT+RzAtpfQ/ZrYZcBkwDLgb+I+UUvFeZh3negF4A3ixBXUfiKxD3Fs7EvfWngymexuRUlq37OBladqxtxozm9uVsJ12Iu6tPYl7a0/i3sqJlAJBEAQVIzr2IAiCitEfHfuUfrhmXxH31p7EvbUncW8l9LnGHgRBEPQuIcUEQRBUjOjYgyAIKkafduxm9nkze9jMFpjZ5L68dqsxs43NbLaZza+nMz62Xj7MzGbW0xnPNLO1mp1rIFLPD3S3mV1X/7sSaZrNbKiZXWFmD9Xf3a4VemfH19vi/WZ2aT3ldlu+NzO70MyWmNn9Ulb4nqzG2fV+ZZ6Z7dh/NW9Oyb2dXm+T88zsal8UWv/spPq9PWxmn+vMNfqsYzezFYBzgS8AWwGHm9lWfXX9XuBd4L9SSiOppViYWL+fycCslNLmwKz63+3IsdRWGDunAWfU72spMK5fatVzzgL+lFLaEtiO2j22/Tszsw2BY4BRKaVtqC0oHEP7vreLgM8vU1b2nr4AbF7/bzyw3PThA4CL+OC9zQS2SSltCzwCnARQ71PGAFvXv3NevS9dLn05Yt8JWJBSejyl9Da1Vauj+/D6LSWltDildFfdfo1aB7EhtXu6uH5YW6YzNrONgC8CF9T/NiqQptnM1gT2BKYCpJTeruc/avt3VmdFYLV6DqfVgcW06XtLKd0EvLRMcdl7Gg38tp5i/DZqeazW75uadp2ie0sp3SDZcm+jln8Lavd2WUrprZTSE8ACan3pcunLjn1D4Gn5uzTVb7thZpsAOwC3A8NTSouh1vkD6/VfzbrNmcCJwPv1v9emk2maBzibAS8Av6nLTBeY2RpU4J2llJ4B/hdYSK1DfwW4k2q8N6fsPVWtbzkauL5ud+ve+rJjt4Kyto+1NLMhwJXAcSmlV/u7Pj3FzA4ClqSU7tTigkPb8d2tCOwInJ9S2oFa3qK2k12KqOvNo4FNgQ2ANahJFMvSju+tGVVpn5jZ96jJvJd4UcFhTe+tLzv2RcDG8nfbp/qtbxV4JXBJSsl3cX7e3cD6v0v6q37dZHfgy2b2JDW5bF9qI/gqpGleBCxKKd1e//sKah19u78zqGVdfSKl9EJK6R3gKmA3qvHenLL3VIm+xczGAgcBR6SOBUbdure+7Nj/AWxen6VfmdqEwPQ+vH5LqevOU4H5KaVfykfTqaUxhjZMZ5xSOimltFFKaRNq7+jGlNIRVCBNc0rpOeBpM9uiXrQf8CBt/s7qLAR2MbPV623T763t35tQ9p6mA0fVo2N2AV5xyaZdMLPPA5OAL6eU3pSPpgNjzGwVM9uU2gTxHU1PmFLqs/+AA6nN+D4GfK8vr90L97IHNZdoHnBP/b8DqenRs4BH6/8O6++69uAe9wauq9ub1RvUAuByYJX+rl8372l7YG79vV0DrFWVdwacAjwE3A/8DlilXd8bcCm1uYJ3qI1ax5W9J2pyxbn1fuU+apFB/X4PXby3BdS0dO9LfiXHf69+bw8DX+jMNSKlQBAEQcWIladBEAQVIzr2IAiCihEdexAEQcWIjj0IgqBiRMceBEFQMaJjD4IgqBjRsQdBEFSM/wftfx/1XZwsZAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# create grid of images\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "# show images\n",
    "matplotlib_imshow(img_grid, one_channel=True)\n",
    "\n",
    "# write to tensorboard\n",
    "#writer.add_image('four_fake_images', img_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "TensorBoard logging requires TensorBoard with Python summary writer installed. This should be available in 1.14 or above.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/meng/lib/python3.7/site-packages/torch/utils/tensorboard/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_writer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRecordWriter\u001b[0m  \u001b[0;31m# noqa F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorboard'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-e911e103ab04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorboard\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# default `log_dir` is \"runs\" - we'll be more specific here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# writer = SummaryWriter(comment=f'2_Bigdataset_test1-Optimisation-Xavier  batch_size={batch_size} lr={lr}')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/meng/lib/python3.7/site-packages/torch/utils/tensorboard/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_writer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRecordWriter\u001b[0m  \u001b[0;31m# noqa F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     raise ImportError('TensorBoard logging requires TensorBoard with Python summary writer installed. '\n\u001b[0m\u001b[1;32m      5\u001b[0m                       'This should be available in 1.14 or above.')\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFileWriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSummaryWriter\u001b[0m  \u001b[0;31m# noqa F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: TensorBoard logging requires TensorBoard with Python summary writer installed. This should be available in 1.14 or above."
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "# writer = SummaryWriter(comment=f'2_Bigdataset_test1-Optimisation-Xavier  batch_size={batch_size} lr={lr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the flex weights before training\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.suptitle('Flexilayer weights before training', fontsize = 16)\n",
    "for idx, filt  in enumerate(net.flex1.weight.detach().numpy()[:,0,:,:]):\n",
    "    plt.subplot(3,4, idx + 1)\n",
    "    plt.imshow(net.flex1.weight.detach().numpy()[idx,0,:,:], cmap=\"gray\")\n",
    "\n",
    "    \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_loss = 0.0\n",
    "plt.figure(figsize=(10,10))\n",
    "for epoch in range(9):  # loop over the dataset multiple times\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    \n",
    "    nsamples = 1000\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        \n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        \n",
    "        preds = net(inputs) # Pass batch\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * batch_size\n",
    "        total_correct += get_num_correct(preds, labels)\n",
    "    \n",
    "        if i == len(trainloader)-1:\n",
    "            net.flex1.memorized_1loop = net.flex1.threshold1\n",
    "            # every epoch plot threshold\n",
    "            title = 'T'+str(epoch)\n",
    "            plt.subplot(5,5, epoch + 1)\n",
    "            plt.subplots_adjust(hspace=0.4, bottom=0.2)\n",
    "            plt.title(title)\n",
    "            #imgplot = plt.imshow(net.flex1.threshold1.detach().numpy()[1,1,:])\n",
    "            imgplot = plt.imshow(net.flex1.memorized_1loop.detach().numpy()[1,1,:] - memorized.detach().numpy()[1,1,:])\n",
    "            \n",
    "    # ...log the total epoch accuracy\n",
    "    # writer.add_scalar('Accuracy', total_correct / len(trainset), epoch)\n",
    "            \n",
    "    print(\"epoch:\", epoch, \"loss:\", total_loss)\n",
    "        \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy is ', total_correct / len(trainset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#before training\n",
    "imgplot = plt.imshow(memorized.detach().numpy()[1,1,:])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after training\n",
    "imgplot = plt.imshow(net.flex1.threshold1.detach().numpy()[1,1,:])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after training\n",
    "imgplot = plt.imshow(net.flex1.threshold1.detach().numpy()[1,1,:] - memorized.detach().numpy()[1,1,:])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after training single training loop\n",
    "imgplot = plt.imshow(net.flex1.threshold1.detach().numpy()[1,1,:] - net.flex1.memorized_1loop.detach().numpy()[1,1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the flex weights after training\n",
    "\n",
    "fig = plt.figure()\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.suptitle('Flexilayer weights after training', fontsize = 16)\n",
    "for idx, filt  in enumerate(net.flex1.weight.detach().numpy()[:,0,:,:]):\n",
    "    im = plt.subplot(3,4, idx + 1)\n",
    "    plt.imshow(net.flex1.weight.detach().numpy()[idx,0,:,:], cmap=\"gray\")\n",
    "#plt.colorbar()\n",
    "\n",
    "    \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
